# -*- coding: utf-8 -*-
"""Traffic sign classification_Training code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ej3L3Qyk4OTdxni-oyFawIAjRAwCCuJ2
"""

from google.colab import files
uploaded = files.upload()

#Upload labels file

from google.colab import files
uploaded = files.upload()

import zipfile
import os

with zipfile.ZipFile("myData.zip", "r") as zip_ref:
    zip_ref.extractall("myData")  # Extract to "images/" folder

print("Done extracting!")

import pandas as pd

# Replace 'yourfile.csv' with your actual file name
df = pd.read_csv("labels.csv")
df.head()

import numpy as np
import os
import pandas as pd
import random

#!pip install opencv-python
import cv2

from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

"""file paths"""

#file_path = "C:/Users/yasin/Desktop/Practice code/Computer vision/Traffic Sign detection/myData/myData"
#csv_path = "C:/Users/yasin/Desktop/Practice code/Computer vision/Traffic Sign detection/labels.csv"

file_path = "/content/myData/myData"       # Assuming you uploaded the 'myData' folder
csv_path = "/content/labels.csv"    # Assuming you uploaded 'labels.csv'

"""### Opening image files"""

count = 0

countClasses = os.listdir(file_path) #lists all dir in file path

imagePaths=[] # will store all img paths
imageClass=[] # will store all class paths (dir)

for i in range(0, len(countClasses)):
    classPath = os.path.join(file_path, str(count))

    #print ("Class path=",classPath)

    for j in os.listdir(classPath):
        imgLink = os.path.join(classPath, str(j))
        img = cv2.imread(imgLink)

        imagePaths.append(img)
        imageClass.append(count)


    count += 1

#print(imageClass)

imagePaths = np.array(imagePaths)
imageClass = np.array(imageClass)

"""### Parameters"""

testRatio = 0.2
validateRatio = 0.2

image_dimensions = (32,32,3)
batch_size_val = 50
steps_per_epoch_val=2000
epochs_val=10

"""Test - Train split"""

x_train, x_test, y_train, y_test = train_test_split(imagePaths, imageClass, test_size= testRatio)
x_train, x_validate, y_train, y_validate = train_test_split(x_train, y_train, test_size = validateRatio)

data = pd.read_csv(csv_path)

plt.figure(figsize=(12,10))

classCounts = np.bincount(imageClass)

plt.bar(range(len(classCounts)), classCounts, color = 'blue' )
plt.xticks(range(len(classCounts)))
plt.title("class vs no of instances")

"""### Preprocessing The images"""

def grayscale(img):
    if len(img.shape) == 3:
        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    return img

def equalize(img):
    #Ensure the image is uint8 (OpenCV expects this for histogram equalization)
    if img.dtype != np.uint8:
        img = (img * 255).astype(np.uint8)
    return cv2.equalizeHist(img)


def preprocessing(img):
    img = grayscale(img)
    img = equalize(img)
    img = img/255.0
    return img

x_train= np.array(list(map(preprocessing,x_train)))
x_validate = np.array(list(map(preprocessing, x_validate)))
x_test = np.array(list(map(preprocessing, x_test)))

x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)
x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], x_test.shape[2], 1)
x_validate = x_validate.reshape(x_validate.shape[0], x_validate.shape[1], x_validate.shape[2], 1)

!pip install tensorflow

from tensorflow.keras.preprocessing.image import ImageDataGenerator

dataGen= ImageDataGenerator(width_shift_range=0.1,   # 0.1 = 10%     IF MORE THAN 1 E.G 10 THEN IT REFFERS TO NO. OF  PIXELS EG 10 PIXELS
                            height_shift_range=0.1,
                            zoom_range=0.2,  # 0.2 MEANS CAN GO FROM 0.8 TO 1.2
                            shear_range=0.1,  # MAGNITUDE OF SHEAR ANGLE
                            rotation_range=10)  # DEGREES

dataGen.fit(x_train)
batches= dataGen.flow(x_train,y_train,batch_size=20)  # REQUESTING DATA GENRATOR TO GENERATE IMAGES  BATCH SIZE = NO. OF IMAGES CREAED EACH TIME ITS CALLED
x_batch,y_batch = next(batches)

# TO SHOW AGMENTED IMAGE SAMPLES
fig,axs=plt.subplots(1,15,figsize=(20,5))
fig.tight_layout()

for i in range(15):
    axs[i].imshow(x_batch[i])
    axs[i].axis('off')
plt.show()

from keras.utils import to_categorical

noOfClasses = len(classCounts)

# Reshape y_train, y_validate, y_test to 1D before one-hot encoding
y_train = y_train.reshape(-1)  # Reshape to 1D
y_validate = y_validate.reshape(-1)  # Reshape to 1D
y_test = y_test.reshape(-1)  # Reshape to 1D

y_train = to_categorical(y_train, noOfClasses)
y_validate = to_categorical(y_validate, noOfClasses)
y_test = to_categorical(y_test, noOfClasses)

"""### Making CNN"""

from keras.layers import Dropout, Flatten
from keras.layers import Conv2D, MaxPooling2D
from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import Adam


def myModel():
    noOfFilters = 50
    sizeOfFilter1 = (5,5)
    sizeOfFilter2 = (3,3)
    sizeOfPool = (2,2)
    noOfNodes = 500

    model = Sequential()

    model.add(Conv2D(noOfFilters, sizeOfFilter1, input_shape= (image_dimensions[0], image_dimensions[1], 1 ), activation = 'relu'))
    model.add(Conv2D(noOfFilters,sizeOfFilter1, activation = 'relu'))
    model.add(MaxPooling2D(pool_size = sizeOfPool))

    model.add((Conv2D(noOfFilters//2, sizeOfFilter2,activation='relu')))
    model.add((Conv2D(noOfFilters // 2, sizeOfFilter2, activation='relu')))
    model.add(MaxPooling2D(pool_size=sizeOfPool))
    model.add(Dropout(0.5))

    model.add(Flatten())
    model.add(Dense(noOfNodes,activation='relu'))
    model.add(Dropout(0.5)) # INPUTS NODES TO DROP WITH EACH UPDATE 1 ALL 0 NONE
    model.add(Dense(noOfClasses,activation='softmax')) # OUTPUT LAYER

    # COMPILE MODEL
    model.compile(Adam(learning_rate= 0.001),loss='categorical_crossentropy',metrics=['accuracy'])

    return model

model = myModel()
print(model.summary())

history = model.fit(
    dataGen.flow(x_train, y_train, batch_size=batch_size_val),
    steps_per_epoch=steps_per_epoch_val,
    epochs=epochs_val,
    validation_data=(x_validate, y_validate),
    shuffle=True
)

"""Plot figures"""

plt.figure(1)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.legend(['training','validation'])
plt.title('loss')
plt.xlabel('epoch')
plt.figure(2)
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.legend(['training','validation'])
plt.title('Acurracy')
plt.xlabel('epoch')
plt.show()
score =model.evaluate(x_test,y_test,verbose=0)
print('Test Score:',score[0])
print('Test Accuracy:',score[1])

import pickle

# STORE THE MODEL AS A PICKLE OBJECT
pickle_out= open("model_trained.p","wb")  # wb = WRITE BYTE
pickle.dump(model,pickle_out)
pickle_out.close()
cv2.waitKey(0)

model.save("model_trained.keras")
from google.colab import files
files.download("model_trained.keras")